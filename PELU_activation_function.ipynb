{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fraquia/PELU-custom-activation-function/blob/main/PELU_activation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnnRxTNdyW1"
      },
      "source": [
        "# Implementing a custom activation function\n",
        "\n",
        "**Author**: *Emanuele Fratocchi*\n",
        "\n",
        "**Mail**: *emanuele.fratocchi90@gmail.com*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEr8qV6-nMuL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAEgygyPfO7b"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The **exponential linear unit** (ELU) is an activation function defined as [1]:\n",
        "\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "x & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        "\\alpha \\left(\\exp\\left(x\\right)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a hyper-parameter. The function is implemented in `tf.keras.layers.ELU` (see the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)).\n",
        "\n",
        "The **parametric ELU** (PELU) extends the ELU activation function as [2]:\n",
        "\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "\\frac{\\alpha}{\\beta}x & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        "\\alpha \\left(\\exp\\Bigl(\\frac{x}{\\beta}\\Bigr)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$\n",
        "\n",
        "where the major difference is that $\\alpha,\\beta > 0$ are *trainable* parameters, i.e., a pair of $(\\alpha, \\beta)$ values is trained for each unit in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u4aF6Z4maHd"
      },
      "source": [
        "### Implement the PELU\n",
        "\n",
        "In TensorFlow, it is possible to implement new layers by subclassing `tf.keras.layers.Layer`:\n",
        "\n",
        "+ [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
        "+ [Custom layers](https://www.tensorflow.org/tutorials/customization/custom_layers)\n",
        "+ [tf.keras.layers.Layer (documentation)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfCpFg2xdug_"
      },
      "outputs": [],
      "source": [
        "class PELU(tf.keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(PELU, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape): \n",
        "\n",
        "        alpha_init = tf.keras.initializers.RandomUniform(minval=3, maxval=None, seed=123)                            #se inziailizzo tropppo basso non funziona nel fit\n",
        "        self.alpha = tf.Variable(\n",
        "            initial_value=alpha_init(shape=(self.units,), dtype=\"float32\"), #self.units,\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        beta_init = tf.keras.initializers.RandomUniform(minval=3, maxval=None, seed=123)\n",
        "        self.beta = tf.Variable(\n",
        "            initial_value=beta_init(shape=(self.units,), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "      output = tf.where(inputs>=0, tf.math.multiply(tf.math.divide(self.alpha, self.beta),inputs), tf.math.multiply(tf.math.subtract(tf.math.exp(tf.math.minimum(tf.math.divide(inputs, self.beta),0)),1),self.alpha))\n",
        "\n",
        "      return output\n",
        "\n",
        "    def get_config(self):             #to save configuration\n",
        "        return {\"units\": self.units} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QY0TfCxo9yd"
      },
      "source": [
        "### Plotting the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "jdK0CyscfDtC",
        "outputId": "77af1c10-8b1c-496c-98a1-d4e8c81885d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer pelu is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd8564076d8>]"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_range = tf.linspace(-5, 5, 200) # An equispaced grid of 200 points in [-5, +5]\n",
        "\n",
        "y_range = tf.convert_to_tensor(pelu(x_range)) \n",
        "\n",
        "\n",
        "plt.plot(x_range.numpy(), y_range.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyGIlR_aqayc"
      },
      "source": [
        "The derivative of a PELU function with respect to the $\\alpha$ parameter is given by [2]:\n",
        "\n",
        "$$\n",
        "\\frac{d\\phi(x)}{d\\alpha} =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "\\frac{x}{\\beta} & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        " \\left(\\exp\\Bigl(\\frac{x}{\\beta}\\Bigr)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnvkoRVAqwg1"
      },
      "source": [
        "Now using a `tf.GradientTape` object, I compute the derivative above using automatic differentiation, and I check its correctness up to a certain numerical precision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muu6nzlEpNl7",
        "outputId": "0d287f97-5b4a-4880-eab7-fc381b534551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7fd8371961e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "#looking to Advanced Automatic Differentiation \n",
        "\n",
        "delta = pelu.alpha #needed because the derivative is for alpha\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y=pelu(x_range)\n",
        "\n",
        "dy_dx = tape.jacobian(y, delta)\n",
        "grads = tf.reshape(dy_dx, shape=(200,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik7pQOp9xg26",
        "outputId": "e86316d8-9514-4f7b-d403-d6d5a6cbccfe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "execution_count": 201,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "beta=pelu.beta[0].numpy() # initial beta value \n",
        "\n",
        "res_list = []                                  #here are saved numerical results of application of der function of each x value\n",
        "for j in x_range:\n",
        "    if j<0:\n",
        "      res_list.append([math.exp(j/beta)-1]) \n",
        "    else:\n",
        "      res_list.append([j/beta])\n",
        "\n",
        "der = tf.convert_to_tensor(res_list, dtype='float32') #convert result list to tensor type\n",
        "\n",
        "tf.reduce_all(tf.abs(grads - res_list) < 1e-4)  #here is checked if two calculus method are similar, if yes return TRUE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I also tried the same apporach but now deriving wrt 𝛽**"
      ],
      "metadata": {
        "id": "ur1F0qzkG_-U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8TC9ua16BpC",
        "outputId": "5313a52f-e9df-4264-b9f3-972263cfa9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7fd837207d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "delta = pelu.beta  #needed because the derivative is for beta\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y=pelu(x_range)\n",
        "\n",
        "dy_dx1 = tape.jacobian(y, delta)\n",
        "grads1 = tf.reshape(dy_dx1, shape=(200,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjQObeqM0ZnW",
        "outputId": "8ac602ed-f25a-491a-be8e-053108068c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "execution_count": 202,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "b=pelu.beta[0].numpy() # initial beta value \n",
        "a=pelu.alpha[0].numpy() # initial beta value \n",
        "\n",
        "res_list1 = []\n",
        "for j in x_range:\n",
        "  if j >=0:\n",
        "    res_list1.append([(-a*j)/(b**2)])\n",
        "  else:\n",
        "\n",
        "    res_list1.append([-a * ((j/(b**2))) * math.exp(j/b)])\n",
        "\n",
        "der1 = tf.convert_to_tensor(res_list1, dtype='float32') #convert result list to tensor type\n",
        "tf.reduce_all(tf.abs(grads1 - der1) < 1e-4)  #here is checked if two calculus method are similar, if yes return TRUE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjnVl6Ysm7F"
      },
      "source": [
        "### PELU in practice\n",
        "\n",
        "I Considered a simple model built with the PELU activation function, as below. I used data from \"Mobile Price Classification\" competion on Kaggle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4YUQA_Inh-_"
      },
      "outputs": [],
      "source": [
        "#let's call all lybrary needed to deal with our dataset and our model\n",
        "from sklearn import model_selection\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import optimizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RDDgr6l5Q26"
      },
      "outputs": [],
      "source": [
        "data_train = pd.read_csv('train_smarphone.csv') #Train set we have to split "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv0FIfmTYPO7"
      },
      "outputs": [],
      "source": [
        "data_train = data_train.to_numpy()  #to split and work on it\n",
        "\n",
        "x_tr = data_train[:,:20] #train\n",
        "y_tr = data_train[:,-1]  #train\n",
        "y_tr = y_tr.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NnGM-VPc5Y3"
      },
      "outputs": [],
      "source": [
        "data_test = pd.read_csv('test_smartphone.csv')\n",
        "\n",
        "col_list = []\n",
        "for i,col in data_test.iteritems():  #problem is one column with no label and all Nan values\n",
        "  if i != 'Unnamed: 0':\n",
        "    col_list.append(i)\n",
        "\n",
        "data_test = data_test[col_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637OMs0eYenm"
      },
      "outputs": [],
      "source": [
        "x_t = data_test.to_numpy()     #test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be9F7aiLgpl7"
      },
      "outputs": [],
      "source": [
        "x_tr = tf.keras.utils.normalize(x_tr, axis =1) #scaled from 0 to 1 to simplify computation \n",
        "x_t = tf.keras.utils.normalize(x_t, axis =1)   #scaled from 0 to 1 to simplify computation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMbiPGe9gpsS"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(layers=[\n",
        "      tf.keras.layers.Dense(20, activation='relu'),            #first layer\n",
        "      tf.keras.layers.Dense(30),                               #second layer\n",
        "      PELU(30),                                                #third layer\n",
        "      tf.keras.layers.Dense(4, activation='softmax')           #output layer\n",
        "])\n",
        "\n",
        "adam = optimizers.Adamax(learning_rate=1e-2)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam ,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqlEZtpBjNzG",
        "outputId": "120adeeb-a121-4f91-f18b-9e1d37720e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 22.2332 - accuracy: 0.4590\n",
            "Epoch 2/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 6.0581 - accuracy: 0.5240\n",
            "Epoch 3/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 4.1471 - accuracy: 0.5415\n",
            "Epoch 4/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 3.6709 - accuracy: 0.5670\n",
            "Epoch 5/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 2.7542 - accuracy: 0.5830\n",
            "Epoch 6/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 2.6588 - accuracy: 0.5915\n",
            "Epoch 7/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 2.2519 - accuracy: 0.5890\n",
            "Epoch 8/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.9768 - accuracy: 0.6210\n",
            "Epoch 9/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 2.2934 - accuracy: 0.6110\n",
            "Epoch 10/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 2.0035 - accuracy: 0.6345\n",
            "Epoch 11/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 2.4326 - accuracy: 0.6275\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd83710d400>"
            ]
          },
          "execution_count": 207,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x =x_tr,y = y_tr,epochs=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K99tLuyIDP1u",
        "outputId": "a64ad1cb-35c9-433b-d9ef-03f39c2bc17f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_62 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 30)                630       \n",
            "_________________________________________________________________\n",
            "pelu_19 (PELU)               (None, 30)                60        \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 4)                 124       \n",
            "=================================================================\n",
            "Total params: 1,234\n",
            "Trainable params: 1,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wBZQD6zFT7G"
      },
      "source": [
        "Here is showed my prediction w.r.t the right label, just to have a simple visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "FnED6FpPprBG",
        "outputId": "b17a36ed-f720-411c-fb01-8dd0d327fecd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x_pred</th>\n",
              "      <th>y_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   x_pred  y_label\n",
              "0       2        1\n",
              "1       1        2\n",
              "2       2        2\n",
              "3       2        2\n",
              "4       1        1\n",
              "5       0        1\n",
              "6       3        3\n",
              "7       0        0\n",
              "8       1        0\n",
              "9       0        0"
            ]
          },
          "execution_count": 211,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = []\n",
        "for el in (model.predict(x_tr)):\n",
        "  prediction.append(np.argmax(el))\n",
        "\n",
        "comp_dataframe = pd.DataFrame()\n",
        "comp_dataframe['x_pred']=prediction\n",
        "comp_dataframe['y_label']=y_tr\n",
        "\n",
        "comp_dataframe.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBcJi3RDQBWF"
      },
      "source": [
        "Now let's do the comparison of performances when we apply ReLU instead of PELU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9If-NOnhSYuN",
        "outputId": "79c145a2-d9fe-45ea-97c4-2d7cdf46cc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 4.7863 - accuracy: 0.3610\n",
            "Epoch 2/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.2844 - accuracy: 0.4780\n",
            "Epoch 3/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.1957 - accuracy: 0.4960\n",
            "Epoch 4/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.2153 - accuracy: 0.4580\n",
            "Epoch 5/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.1166 - accuracy: 0.4875\n",
            "Epoch 6/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.0565 - accuracy: 0.5035\n",
            "Epoch 7/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.0168 - accuracy: 0.5225\n",
            "Epoch 8/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.9615 - accuracy: 0.5315\n",
            "Epoch 9/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.9531 - accuracy: 0.5475\n",
            "Epoch 10/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.9534 - accuracy: 0.5630\n",
            "Epoch 11/11\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.9106 - accuracy: 0.5615\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd8371b2c50>"
            ]
          },
          "execution_count": 198,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_relu = tf.keras.Sequential(layers=[\n",
        "      tf.keras.layers.Dense(20, activation='relu'),            #first layer\n",
        "      tf.keras.layers.Dense(30),                               #second layer\n",
        "      tf.keras.layers.ReLU(30),                                 #third layer\n",
        "      tf.keras.layers.Dense(4, activation='softmax')           #output layer\n",
        "])\n",
        "\n",
        "adam = optimizers.Adamax(learning_rate=1e-2)\n",
        "\n",
        "model_relu.compile(loss='sparse_categorical_crossentropy', optimizer=adam ,metrics=['accuracy'])\n",
        "\n",
        "model_relu.fit(x =x_tr,y = y_tr,epochs=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2CDnlX8EsGq"
      },
      "source": [
        "As expected usign Relu on same dataset is shown a lower accuracy performance. We know that basically they have same behaviour as activation functions, acting as linear function when input values are greater than 0. The difference on performances can be underlined knowing that Pelu has alpha and beta parametres that can be trained. This leads to Pelu to fit and perform better on our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ScwTToSY3k",
        "outputId": "4dc36fb2-25de-4128-c78b-e6300e2ff727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_57 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 30)                630       \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 4)                 124       \n",
            "=================================================================\n",
            "Total params: 1,174\n",
            "Trainable params: 1,174\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_relu.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl9dZ3HambSz"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] Clevert, D.A., Unterthiner, T. and Hochreiter, S., 2015. [Fast and accurate deep network learning by exponential linear units (ELUs)](https://arxiv.org/abs/1511.07289). arXiv preprint arXiv:1511.07289.\n",
        "\n",
        "[2] Trottier, L., Gigu, P. and Chaib-draa, B., 2017. [Parametric exponential linear unit for deep convolutional neural networks](https://arxiv.org/abs/1605.09332). In 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA) (pp. 207-214). IEEE."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wAEgygyPfO7b",
        "Rl9dZ3HambSz"
      ],
      "name": "custom_activation_function.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}